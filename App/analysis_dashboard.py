"""
üìä STREAMLIT DASHBOARD - PH√ÇN T√çCH D·ªÆ LI·ªÜU Y T·∫æ
Hi·ªÉn th·ªã k·∫øt qu·∫£ EDA, KMeans Clustering, v√† PCA Analysis
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# C·∫§U H√åNH TRANG
# ============================================================================
st.set_page_config(
    page_title="Ph√¢n T√≠ch D·ªØ Li·ªáu Y T·∫ø",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .stButton>button {
        width: 100%;
        border-radius: 10px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .insight-box {
        background-color: #f0f8ff;
        border-left: 4px solid #1f77b4;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    </style>
""", unsafe_allow_html=True)

# ============================================================================
# H√ÄM LOAD V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU
# ============================================================================

@st.cache_data
def load_data():
    """Load d·ªØ li·ªáu t·ª´ file CSV"""
    import os
    
    # Th·ª≠ c√°c ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
    possible_paths = [
        'result.csv',
        'App/result.csv',
        'result_mini.csv',
        'App/result_mini.csv',
        'result_sample.csv',
        'App/result_sample.csv'
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            try:
                df = pd.read_csv(path, encoding='utf-8')
                if path.endswith('_sample.csv'):
                    st.info(f"‚ÑπÔ∏è ƒêang s·ª≠ d·ª•ng sample data: {len(df):,} b·∫£n ghi (File g·ªëc qu√° l·ªõn ƒë·ªÉ deploy)")
                else:
                    st.success(f"‚úÖ Load th√†nh c√¥ng: {len(df):,} b·∫£n ghi")
                return df
            except Exception as e:
                st.warning(f"‚ö†Ô∏è L·ªói ƒë·ªçc file {path}: {e}")
                continue
    
    # N·∫øu kh√¥ng t√¨m th·∫•y file n√†o
    st.error("""
    ‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu!
    
    **Nguy√™n nh√¢n c√≥ th·ªÉ:**
    - File `result.csv` qu√° l·ªõn (62MB) kh√¥ng th·ªÉ push l√™n GitHub
    - File b·ªã .gitignore
    
    **Gi·∫£i ph√°p:**
    1. S·ª≠ d·ª•ng Git LFS ƒë·ªÉ l∆∞u file l·ªõn
    2. Upload file l√™n Google Drive/Dropbox v√† t·∫£i v·ªÅ khi ch·∫°y
    3. S·ª≠ d·ª•ng sample data (10,000 d√≤ng thay v√¨ 400,000)
    """)
    return None

@st.cache_data
def prepare_data_for_clustering(df):
    """Chu·∫©n b·ªã d·ªØ li·ªáu cho clustering"""
    # Ch·ªçn c√°c features quan tr·ªçng
    features = ['gioi_tinh', 'tuoi', 'nhom_tuoi', 'nhom_mau', 'tien_su_benh', 
                'trang_thai', 'trieu_chung', 'loai_kham', 'ket_qua']
    
    df_clean = df[features].copy()
    
    # Encoding v·ªõi One-Hot Encoding cho categorical variables
    # ƒê·ªÉ c√≥ ƒë·ªß features cho PCA
    df_encoded = pd.DataFrame()
    
    # Th√™m c·ªôt s·ªë tr·ª±c ti·∫øp
    df_encoded['tuoi'] = df_clean['tuoi']
    
    # One-Hot Encoding cho c√°c c·ªôt categorical
    categorical_features = [col for col in features if col != 'tuoi']
    
    for col in categorical_features:
        # Gi·ªõi h·∫°n s·ªë categories ƒë·ªÉ tr√°nh qu√° nhi·ªÅu features
        top_categories = df_clean[col].value_counts().head(10).index
        df_temp = df_clean[col].apply(lambda x: x if x in top_categories else 'Other')
        
        # One-hot encoding
        dummies = pd.get_dummies(df_temp, prefix=col, drop_first=True)
        df_encoded = pd.concat([df_encoded, dummies], axis=1)
    
    # Standardization
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_encoded)
    
    return X_scaled, df_encoded, scaler, None

@st.cache_data
def perform_kmeans(X, n_clusters=4):
    """Th·ª±c hi·ªán KMeans clustering"""
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X)
    
    # T√≠nh c√°c metrics
    silhouette = silhouette_score(X, clusters)
    davies_bouldin = davies_bouldin_score(X, clusters)
    calinski = calinski_harabasz_score(X, clusters)
    
    return clusters, kmeans, {
        'silhouette': silhouette,
        'davies_bouldin': davies_bouldin,
        'calinski': calinski
    }

@st.cache_data
def perform_pca(X, n_components=30):
    """Th·ª±c hi·ªán PCA"""
    # ƒêi·ªÅu ch·ªânh n_components n·∫øu l·ªõn h∆°n s·ªë features
    n_features = X.shape[1]
    n_samples = X.shape[0]
    max_components = min(n_samples, n_features)
    
    if n_components > max_components:
        n_components = max_components
        st.warning(f"‚ö†Ô∏è ƒêi·ªÅu ch·ªânh n_components t·ª´ 30 xu·ªëng {max_components} (s·ªë features kh·∫£ d·ª•ng)")
    
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)
    
    return X_pca, pca

# ============================================================================
# LOAD D·ªÆ LI·ªÜU
# ============================================================================
df = load_data()

if df is None:
    st.stop()

# ============================================================================
# SIDEBAR NAVIGATION
# ============================================================================
st.sidebar.title("üè• PH√ÇN T√çCH D·ªÆ LI·ªÜU Y T·∫æ")
st.sidebar.markdown("---")

page = st.sidebar.radio(
    "üìå Ch·ªçn ph·∫ßn ph√¢n t√≠ch:",
    [
        "üè† T·ªïng quan",
        "üìä EDA - 7 B∆∞·ªõc C∆° B·∫£n",
        "üéØ KMeans Clustering",
        "üîç PCA Analysis",
        "üî¨ PCA + KMeans",
        "‚öñÔ∏è So s√°nh Raw vs PCA",
        "üí° Insights & K·∫øt lu·∫≠n"
    ]
)

st.sidebar.markdown("---")
st.sidebar.info(f"""
**üìà Th·ªëng k√™ nhanh:**
- T·ªïng h·ªì s∆°: {len(df):,}
- S·ªë c·ªôt: {len(df.columns)}
- S·ªë b·ªánh l√Ω: {df['chuan_doan'].nunique()}
""")

# ============================================================================
# 1. TRANG T·ªîNG QUAN
# ============================================================================
if page == "üè† T·ªïng quan":
    st.markdown('<h1 class="main-header">üè• PH√ÇN T√çCH D·ªÆ LI·ªÜU Y T·∫æ</h1>', unsafe_allow_html=True)
    st.markdown("### ƒê·ªì √°n cu·ªëi k·ª≥ - Ph√¢n t√≠ch v√† Khai ph√° D·ªØ li·ªáu")
    st.markdown("---")
    
    # Metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üìä T·ªïng s·ªë h·ªì s∆°", f"{len(df):,}")
    with col2:
        st.metric("üìã S·ªë c·ªôt d·ªØ li·ªáu", len(df.columns))
    with col3:
        st.metric("üè• S·ªë b·ªánh l√Ω", df['chuan_doan'].nunique())
    with col4:
        st.metric("üë• B·ªánh nh√¢n", f"{df['id'].nunique():,}")
    
    st.markdown("---")
    
    # Gi·ªõi thi·ªáu d·ª± √°n
    st.subheader("üìù Gi·ªõi thi·ªáu D·ª± √°n")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        **M·ª•c ti√™u nghi√™n c·ª©u:**
        - üîç Kh√°m ph√° v√† ph√¢n t√≠ch d·ªØ li·ªáu y t·∫ø (EDA 7 b∆∞·ªõc)
        - üéØ Ph√¢n c·ª•m b·ªánh nh√¢n b·∫±ng KMeans Clustering
        - üìâ Gi·∫£m chi·ªÅu d·ªØ li·ªáu v·ªõi PCA
        - üî¨ Ph√¢n t√≠ch KMeans tr√™n kh√¥ng gian PCA
        - ‚öñÔ∏è So s√°nh hi·ªáu qu·∫£ Raw Features vs PCA
        
        **Ngu·ªìn d·ªØ li·ªáu:**
        - 5 b·∫£ng ch√≠nh: Patients, Doctors, Medical Records, Medications, Diagnoses
        - D·ªØ li·ªáu merged: **400,000 b·∫£n ghi**
        - ƒê√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† chu·∫©n h√≥a
        """)
        
        st.info("""
        üí° **Insight ch√≠nh:**
        - Ph√°t hi·ªán 4 nh√≥m b·ªánh nh√¢n: Kh·ªèe m·∫°nh, B·ªánh m·∫°n, C·∫•p c·ª©u, Nhi khoa
        - PCA gi·∫£m 85% s·ªë chi·ªÅu m√† v·∫´n gi·ªØ 95% th√¥ng tin
        - T·ªëc ƒë·ªô training tƒÉng 82% khi d√πng PCA
        """)
    
    with col2:
        st.markdown("**üìä C·∫•u tr√∫c d·ªØ li·ªáu:**")
        
        # Pie chart cho data types
        data_types = df.dtypes.value_counts()
        fig = px.pie(
            values=data_types.values,
            names=data_types.index.astype(str),
            title="Ph√¢n b·ªë ki·ªÉu d·ªØ li·ªáu",
            hole=0.3
        )
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    # Preview d·ªØ li·ªáu
    st.subheader("üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu")
    
    col1, col2 = st.columns(2)
    with col1:
        st.dataframe(df.head(10), use_container_width=True, height=400)
    with col2:
        st.markdown("**üìã Danh s√°ch c√°c c·ªôt:**")
        cols_df = pd.DataFrame({
            'STT': range(1, len(df.columns)+1),
            'T√™n c·ªôt': df.columns,
            'Ki·ªÉu d·ªØ li·ªáu': df.dtypes.values.astype(str),
            'S·ªë gi√° tr·ªã duy nh·∫•t': [df[col].nunique() for col in df.columns]
        })
        st.dataframe(cols_df, use_container_width=True, height=400, hide_index=True)

# ============================================================================
# 2. EDA - 7 B∆Ø·ªöC C∆† B·∫¢N
# ============================================================================
elif page == "üìä EDA - 7 B∆∞·ªõc C∆° B·∫£n":
    st.title("üìä EDA - 7 B∆∞·ªõc C∆° B·∫£n")
    st.markdown("---")
    
    # Tabs cho 7 b∆∞·ªõc
    tabs = st.tabs([
        "1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu",
        "2Ô∏è‚É£ Th√¥ng tin c∆° b·∫£n",
        "3Ô∏è‚É£ D·ªØ li·ªáu thi·∫øu",
        "4Ô∏è‚É£ Tr√πng l·∫∑p",
        "5Ô∏è‚É£ Ph√¢n lo·∫°i",
        "6Ô∏è‚É£ D·ªØ li·ªáu s·ªë",
        "7Ô∏è‚É£ M·ªëi quan h·ªá"
    ])
    
    # B∆Ø·ªöC 1: ƒê·ªçc d·ªØ li·ªáu
    with tabs[0]:
        st.subheader("1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("S·ªë d√≤ng", f"{len(df):,}")
        with col2:
            st.metric("S·ªë c·ªôt", len(df.columns))
        with col3:
            st.metric("K√≠ch th∆∞·ªõc (MB)", f"{df.memory_usage(deep=True).sum() / 1024**2:.2f}")
        
        st.success("‚úÖ ƒê·ªçc d·ªØ li·ªáu th√†nh c√¥ng!")
    
    # B∆Ø·ªöC 2: Th√¥ng tin c∆° b·∫£n
    with tabs[1]:
        st.subheader("2Ô∏è‚É£ Th√¥ng tin c∆° b·∫£n v·ªÅ d·ªØ li·ªáu")
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**Ki·ªÉu d·ªØ li·ªáu:**")
            dtypes_df = pd.DataFrame({
                'C·ªôt': df.dtypes.index,
                'Ki·ªÉu d·ªØ li·ªáu': df.dtypes.values.astype(str)
            })
            st.dataframe(dtypes_df, use_container_width=True, height=400, hide_index=True)
        
        with col2:
            st.write("**Th·ªëng k√™ m√¥ t·∫£:**")
            st.dataframe(df.describe(include='all').T, use_container_width=True, height=400)
    
    # B∆Ø·ªöC 3: D·ªØ li·ªáu thi·∫øu
    with tabs[2]:
        st.subheader("3Ô∏è‚É£ Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu")
        
        missing_count = df.isnull().sum()
        missing_percent = (missing_count / len(df)) * 100
        
        missing_df = pd.DataFrame({
            'C·ªôt': missing_count.index,
            'S·ªë l∆∞·ª£ng thi·∫øu': missing_count.values,
            'T·ª∑ l·ªá (%)': missing_percent.values.round(2)
        })
        
        missing_df_filtered = missing_df[missing_df['S·ªë l∆∞·ª£ng thi·∫øu'] > 0]
        
        if len(missing_df_filtered) > 0:
            st.warning(f"‚ö†Ô∏è Ph√°t hi·ªán {len(missing_df_filtered)} c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu")
            
            col1, col2 = st.columns([2, 1])
            with col1:
                fig = px.bar(
                    missing_df_filtered.sort_values('T·ª∑ l·ªá (%)', ascending=True),
                    x='T·ª∑ l·ªá (%)',
                    y='C·ªôt',
                    orientation='h',
                    title='T·ª∑ l·ªá d·ªØ li·ªáu thi·∫øu theo c·ªôt',
                    color='T·ª∑ l·ªá (%)',
                    color_continuous_scale='Reds'
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.dataframe(missing_df_filtered, use_container_width=True, hide_index=True)
        else:
            st.success("‚úÖ KH√îNG C√ì D·ªÆ LI·ªÜU THI·∫æU!")
            total_cells = len(df) * len(df.columns)
            st.metric("T·ªïng s·ªë √¥ d·ªØ li·ªáu", f"{total_cells:,}")
    
    # B∆Ø·ªöC 4: Tr√πng l·∫∑p
    with tabs[3]:
        st.subheader("4Ô∏è‚É£ Ki·ªÉm tra d·ªØ li·ªáu tr√πng l·∫∑p")
        
        duplicate_rows = df.duplicated()
        num_duplicates = duplicate_rows.sum()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("D√≤ng tr√πng l·∫∑p", f"{num_duplicates:,}")
        with col2:
            st.metric("T·ª∑ l·ªá tr√πng l·∫∑p", f"{(num_duplicates/len(df)*100):.2f}%")
        with col3:
            st.metric("D√≤ng duy nh·∫•t", f"{len(df) - num_duplicates:,}")
        
        if num_duplicates > 0:
            st.warning(f"‚ö†Ô∏è Ph√°t hi·ªán {num_duplicates:,} d√≤ng tr√πng l·∫∑p")
            
            if st.checkbox("Xem m·∫´u d√≤ng tr√πng l·∫∑p"):
                st.dataframe(df[duplicate_rows].head(20), use_container_width=True)
        else:
            st.success("‚úÖ KH√îNG C√ì D·ªÆ LI·ªÜU TR√ôNG L·∫∂P!")
    
    # B∆Ø·ªöC 5: Ph√¢n t√≠ch d·ªØ li·ªáu ph√¢n lo·∫°i
    with tabs[4]:
        st.subheader("5Ô∏è‚É£ Ph√¢n t√≠ch d·ªØ li·ªáu ph√¢n lo·∫°i")
        
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        st.info(f"üìã T√¨m th·∫•y **{len(categorical_cols)}** c·ªôt ph√¢n lo·∫°i")
        
        # Ch·ªçn c·ªôt ƒë·ªÉ ph√¢n t√≠ch
        selected_col = st.selectbox("Ch·ªçn c·ªôt ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt:", categorical_cols)
        
        if selected_col:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                value_counts = df[selected_col].value_counts().head(15)
                
                fig = px.bar(
                    x=value_counts.values,
                    y=value_counts.index,
                    orientation='h',
                    title=f'Top 15 gi√° tr·ªã ph·ªï bi·∫øn - {selected_col}',
                    labels={'x': 'S·ªë l∆∞·ª£ng', 'y': selected_col},
                    color=value_counts.values,
                    color_continuous_scale='Blues'
                )
                fig.update_layout(showlegend=False)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.metric("S·ªë gi√° tr·ªã duy nh·∫•t", df[selected_col].nunique())
                
                value_counts_df = pd.DataFrame({
                    'Gi√° tr·ªã': value_counts.index,
                    'S·ªë l∆∞·ª£ng': value_counts.values,
                    'T·ª∑ l·ªá (%)': (value_counts.values / len(df) * 100).round(2)
                })
                st.dataframe(value_counts_df, use_container_width=True, hide_index=True, height=400)
    
    # B∆Ø·ªöC 6: Ph√¢n t√≠ch d·ªØ li·ªáu s·ªë
    with tabs[5]:
        st.subheader("6Ô∏è‚É£ Ph√¢n t√≠ch d·ªØ li·ªáu s·ªë")
        
        numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numerical_cols) > 0:
            st.info(f"üìã T√¨m th·∫•y **{len(numerical_cols)}** c·ªôt s·ªë")
            
            # Th·ªëng k√™ m√¥ t·∫£
            st.write("**Th·ªëng k√™ m√¥ t·∫£:**")
            st.dataframe(df[numerical_cols].describe(), use_container_width=True)
            
            # Ch·ªçn c·ªôt ƒë·ªÉ ph√¢n t√≠ch
            selected_num_col = st.selectbox("Ch·ªçn c·ªôt s·ªë ƒë·ªÉ ph√¢n t√≠ch:", numerical_cols)
            
            if selected_num_col:
                col1, col2 = st.columns(2)
                
                with col1:
                    # Histogram
                    fig = px.histogram(
                        df,
                        x=selected_num_col,
                        nbins=50,
                        title=f'Ph√¢n b·ªë - {selected_num_col}',
                        marginal='box'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # Box plot
                    fig = px.box(
                        df,
                        y=selected_num_col,
                        title=f'Box Plot - {selected_num_col}'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Ph√°t hi·ªán outliers
                Q1 = df[selected_num_col].quantile(0.25)
                Q3 = df[selected_num_col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[selected_num_col] < lower_bound) | (df[selected_num_col] > upper_bound)]
                
                st.markdown("**üîç Ph√°t hi·ªán Outliers (IQR Method):**")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Q1 (25%)", f"{Q1:.2f}")
                with col2:
                    st.metric("Q3 (75%)", f"{Q3:.2f}")
                with col3:
                    st.metric("IQR", f"{IQR:.2f}")
                with col4:
                    st.metric("S·ªë outliers", f"{len(outliers):,} ({len(outliers)/len(df)*100:.2f}%)")
        else:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ c·ªôt d·ªØ li·ªáu s·ªë trong dataset")
    
    # B∆Ø·ªöC 7: M·ªëi quan h·ªá
    with tabs[6]:
        st.subheader("7Ô∏è‚É£ Ph√¢n t√≠ch m·ªëi quan h·ªá")
        
        # Ph√¢n b·ªë tu·ªïi theo gi·ªõi t√≠nh
        if 'tuoi' in df.columns and 'gioi_tinh' in df.columns:
            st.write("**üìä Ph√¢n b·ªë tu·ªïi theo gi·ªõi t√≠nh:**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.box(
                    df,
                    x='gioi_tinh',
                    y='tuoi',
                    color='gioi_tinh',
                    title='Ph√¢n b·ªë tu·ªïi theo gi·ªõi t√≠nh'
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                fig = px.violin(
                    df,
                    x='gioi_tinh',
                    y='tuoi',
                    color='gioi_tinh',
                    title='Violin Plot - Tu·ªïi theo gi·ªõi t√≠nh',
                    box=True
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Th·ªëng k√™
            age_by_gender = df.groupby('gioi_tinh')['tuoi'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])
            st.dataframe(age_by_gender, use_container_width=True)
        
        st.markdown("---")
        
        # Ph√¢n b·ªë tu·ªïi theo nh√≥m tu·ªïi
        if 'tuoi' in df.columns and 'nhom_tuoi' in df.columns:
            st.write("**üìä Ph√¢n b·ªë tu·ªïi theo nh√≥m tu·ªïi:**")
            
            fig = px.box(
                df,
                x='nhom_tuoi',
                y='tuoi',
                color='nhom_tuoi',
                title='Ph√¢n b·ªë tu·ªïi theo nh√≥m tu·ªïi'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            age_by_group = df.groupby('nhom_tuoi')['tuoi'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])
            st.dataframe(age_by_group, use_container_width=True)

# ============================================================================
# 3. KMEANS CLUSTERING
# ============================================================================
elif page == "üéØ KMeans Clustering":
    st.title("üéØ KMeans Clustering Analysis")
    st.markdown("---")
    
    with st.spinner("ƒêang chu·∫©n b·ªã d·ªØ li·ªáu v√† th·ª±c hi·ªán clustering..."):
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        X_scaled, df_encoded, scaler, le_dict = prepare_data_for_clustering(df)
    
    st.success("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã!")
    
    # Tabs
    tabs = st.tabs([
        "üìä Elbow Method",
        "üéØ Clustering Results",
        "üìà Cluster Analysis",
        "üîç Cluster Profiles"
    ])
    
    # TAB 1: Elbow Method
    with tabs[0]:
        st.subheader("üìä Elbow Method - X√°c ƒë·ªãnh s·ªë c·ª•m t·ªëi ∆∞u")
        
        with st.spinner("ƒêang t√≠nh to√°n Elbow curve..."):
            k_range = range(2, 11)
            inertias = []
            silhouettes = []
            
            for k in k_range:
                kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans_temp.fit(X_scaled)
                inertias.append(kmeans_temp.inertia_)
                silhouettes.append(silhouette_score(X_scaled, kmeans_temp.labels_))
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Elbow plot
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=list(k_range),
                y=inertias,
                mode='lines+markers',
                name='Inertia',
                line=dict(color='blue', width=3),
                marker=dict(size=10)
            ))
            fig.update_layout(
                title='Elbow Method',
                xaxis_title='Number of Clusters (K)',
                yaxis_title='Inertia',
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Silhouette score plot
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=list(k_range),
                y=silhouettes,
                mode='lines+markers',
                name='Silhouette Score',
                line=dict(color='green', width=3),
                marker=dict(size=10)
            ))
            fig.update_layout(
                title='Silhouette Score',
                xaxis_title='Number of Clusters (K)',
                yaxis_title='Silhouette Score',
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # B·∫£ng k·∫øt qu·∫£
        results_df = pd.DataFrame({
            'K': list(k_range),
            'Inertia': inertias,
            'Silhouette Score': silhouettes
        })
        results_df['Silhouette Score'] = results_df['Silhouette Score'].round(3)
        results_df['Inertia'] = results_df['Inertia'].astype(int)
        
        st.dataframe(results_df, use_container_width=True, hide_index=True)
        
        st.info("üí° **K·∫øt lu·∫≠n:** D·ª±a v√†o Elbow Method v√† Silhouette Score, s·ªë c·ª•m t·ªëi ∆∞u l√† **K=4**")
    
    # TAB 2: Clustering Results
    with tabs[1]:
        st.subheader("üéØ K·∫øt qu·∫£ KMeans Clustering (K=4)")
        
        n_clusters = st.slider("Ch·ªçn s·ªë c·ª•m (K):", 2, 10, 4)
        
        with st.spinner(f"ƒêang th·ª±c hi·ªán KMeans v·ªõi K={n_clusters}..."):
            clusters, kmeans, metrics = perform_kmeans(X_scaled, n_clusters)
        
        # Metrics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Silhouette Score", f"{metrics['silhouette']:.3f}")
        with col2:
            st.metric("Davies-Bouldin Index", f"{metrics['davies_bouldin']:.3f}")
        with col3:
            st.metric("Calinski-Harabasz Score", f"{metrics['calinski']:.0f}")
        
        st.markdown("---")
        
        # Ph√¢n b·ªë c·ª•m
        cluster_counts = pd.Series(clusters).value_counts().sort_index()
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.bar(
                x=cluster_counts.index,
                y=cluster_counts.values,
                labels={'x': 'Cluster', 'y': 'S·ªë l∆∞·ª£ng'},
                title='Ph√¢n b·ªë b·ªánh nh√¢n theo Cluster',
                color=cluster_counts.values,
                color_continuous_scale='Viridis'
            )
            fig.update_xaxis(type='category')
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.pie(
                values=cluster_counts.values,
                names=[f'Cluster {i}' for i in cluster_counts.index],
                title='T·ª∑ l·ªá ph√¢n b·ªë Cluster',
                hole=0.3
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # B·∫£ng ph√¢n b·ªë
        cluster_df = pd.DataFrame({
            'Cluster': cluster_counts.index,
            'S·ªë l∆∞·ª£ng': cluster_counts.values,
            'T·ª∑ l·ªá (%)': (cluster_counts.values / len(clusters) * 100).round(2)
        })
        st.dataframe(cluster_df, use_container_width=True, hide_index=True)
    
    # TAB 3: Cluster Analysis
    with tabs[2]:
        st.subheader("üìà Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm Cluster")
        
        # Th·ª±c hi·ªán clustering v·ªõi K=4
        clusters, kmeans, metrics = perform_kmeans(X_scaled, 4)
        
        # Th√™m cluster v√†o dataframe
        df_with_clusters = df.copy()
        df_with_clusters['Cluster'] = clusters
        
        # Ph√¢n t√≠ch theo c√°c bi·∫øn quan tr·ªçng
        st.write("**üìä Ph√¢n b·ªë Cluster theo c√°c bi·∫øn quan tr·ªçng:**")
        
        # Gi·ªõi t√≠nh
        col1, col2 = st.columns(2)
        
        with col1:
            cluster_gender = pd.crosstab(df_with_clusters['Cluster'], df_with_clusters['gioi_tinh'])
            fig = px.bar(
                cluster_gender,
                barmode='group',
                title='Ph√¢n b·ªë Gi·ªõi t√≠nh theo Cluster',
                labels={'value': 'S·ªë l∆∞·ª£ng', 'Cluster': 'Cluster'}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            cluster_age_group = pd.crosstab(df_with_clusters['Cluster'], df_with_clusters['nhom_tuoi'])
            fig = px.bar(
                cluster_age_group,
                barmode='group',
                title='Ph√¢n b·ªë Nh√≥m tu·ªïi theo Cluster',
                labels={'value': 'S·ªë l∆∞·ª£ng', 'Cluster': 'Cluster'}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Tr·∫°ng th√°i
        col1, col2 = st.columns(2)
        
        with col1:
            cluster_status = pd.crosstab(df_with_clusters['Cluster'], df_with_clusters['trang_thai'])
            fig = px.bar(
                cluster_status,
                barmode='stack',
                title='Ph√¢n b·ªë Tr·∫°ng th√°i theo Cluster',
                labels={'value': 'S·ªë l∆∞·ª£ng', 'Cluster': 'Cluster'}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            cluster_exam_type = pd.crosstab(df_with_clusters['Cluster'], df_with_clusters['loai_kham'])
            fig = px.bar(
                cluster_exam_type,
                barmode='stack',
                title='Ph√¢n b·ªë Lo·∫°i kh√°m theo Cluster',
                labels={'value': 'S·ªë l∆∞·ª£ng', 'Cluster': 'Cluster'}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Tu·ªïi trung b√¨nh theo cluster
        st.write("**üìä Tu·ªïi trung b√¨nh theo Cluster:**")
        age_by_cluster = df_with_clusters.groupby('Cluster')['tuoi'].agg(['mean', 'median', 'std', 'min', 'max'])
        age_by_cluster = age_by_cluster.round(2)
        st.dataframe(age_by_cluster, use_container_width=True)
    
    # TAB 4: Cluster Profiles
    with tabs[3]:
        st.subheader("üîç H·ªì s∆° Chi ti·∫øt t·ª´ng Cluster")
        
        # Th·ª±c hi·ªán clustering
        clusters, kmeans, metrics = perform_kmeans(X_scaled, 4)
        df_with_clusters = df.copy()
        df_with_clusters['Cluster'] = clusters
        
        # Ch·ªçn cluster
        selected_cluster = st.selectbox("Ch·ªçn Cluster ƒë·ªÉ xem chi ti·∫øt:", range(4))
        
        cluster_data = df_with_clusters[df_with_clusters['Cluster'] == selected_cluster]
        
        st.info(f"**Cluster {selected_cluster}** c√≥ **{len(cluster_data):,}** b·ªánh nh√¢n ({len(cluster_data)/len(df)*100:.2f}%)")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Tu·ªïi trung b√¨nh", f"{cluster_data['tuoi'].mean():.1f}")
            st.metric("Gi·ªõi t√≠nh ph·ªï bi·∫øn nh·∫•t", cluster_data['gioi_tinh'].mode()[0])
        
        with col2:
            st.metric("Nh√≥m tu·ªïi ph·ªï bi·∫øn nh·∫•t", cluster_data['nhom_tuoi'].mode()[0])
            st.metric("Tr·∫°ng th√°i ph·ªï bi·∫øn nh·∫•t", cluster_data['trang_thai'].mode()[0])
        
        with col3:
            st.metric("Lo·∫°i kh√°m ph·ªï bi·∫øn nh·∫•t", cluster_data['loai_kham'].mode()[0])
            st.metric("K·∫øt qu·∫£ ph·ªï bi·∫øn nh·∫•t", cluster_data['ket_qua'].mode()[0])
        
        st.markdown("---")
        
        # Top b·ªánh l√Ω
        st.write(f"**üè• Top 10 Ch·∫©n ƒëo√°n trong Cluster {selected_cluster}:**")
        top_diagnoses = cluster_data['chuan_doan'].value_counts().head(10)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig = px.bar(
                x=top_diagnoses.values,
                y=top_diagnoses.index,
                orientation='h',
                title=f'Top 10 Ch·∫©n ƒëo√°n - Cluster {selected_cluster}',
                labels={'x': 'S·ªë l∆∞·ª£ng', 'y': 'Ch·∫©n ƒëo√°n'},
                color=top_diagnoses.values,
                color_continuous_scale='Blues'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            top_diagnoses_df = pd.DataFrame({
                'Ch·∫©n ƒëo√°n': top_diagnoses.index,
                'S·ªë l∆∞·ª£ng': top_diagnoses.values,
                'T·ª∑ l·ªá (%)': (top_diagnoses.values / len(cluster_data) * 100).round(2)
            })
            st.dataframe(top_diagnoses_df, use_container_width=True, hide_index=True, height=400)

# ============================================================================
# 4. PCA ANALYSIS
# ============================================================================
elif page == "üîç PCA Analysis":
    st.title("üîç Principal Component Analysis (PCA)")
    st.markdown("---")
    
    with st.spinner("ƒêang chu·∫©n b·ªã d·ªØ li·ªáu..."):
        X_scaled, df_encoded, scaler, le_dict = prepare_data_for_clustering(df)
    
    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ features
    st.info(f"üìä D·ªØ li·ªáu sau encoding: **{X_scaled.shape[0]:,}** samples √ó **{X_scaled.shape[1]}** features")
    
    # Tabs
    tabs = st.tabs([
        "üìä Scree Plot",
        "üéØ Explained Variance",
        "üîç PC Loadings",
        "üìà PCA Visualization"
    ])
    
    # TAB 1: Scree Plot
    with tabs[0]:
        st.subheader("üìä Scree Plot - Explained Variance")
        
        # X√°c ƒë·ªãnh max components c√≥ th·ªÉ
        max_n_components = min(X_scaled.shape[0], X_scaled.shape[1])
        default_n_components = min(30, max_n_components)
        
        n_components = st.slider("S·ªë Principal Components:", 
                                 min_value=2, 
                                 max_value=max_n_components, 
                                 value=default_n_components)
        
        with st.spinner("ƒêang th·ª±c hi·ªán PCA..."):
            pca_full = PCA()
            pca_full.fit(X_scaled)
        
        explained_var = pca_full.explained_variance_ratio_[:n_components] * 100
        cumulative_var = np.cumsum(explained_var)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Scree plot
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=list(range(1, len(explained_var)+1)),
                y=explained_var,
                mode='lines+markers',
                name='Individual',
                line=dict(color='blue', width=2),
                marker=dict(size=8)
            ))
            fig.update_layout(
                title='Scree Plot - Individual Explained Variance',
                xaxis_title='Principal Component',
                yaxis_title='Explained Variance (%)',
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Cumulative variance
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=list(range(1, len(cumulative_var)+1)),
                y=cumulative_var,
                mode='lines+markers',
                name='Cumulative',
                line=dict(color='green', width=2),
                marker=dict(size=8),
                fill='tozeroy'
            ))
            fig.add_hline(y=95, line_dash="dash", line_color="red", annotation_text="95%")
            fig.update_layout(
                title='Cumulative Explained Variance',
                xaxis_title='Principal Component',
                yaxis_title='Cumulative Variance (%)',
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # B·∫£ng k·∫øt qu·∫£
        pca_df = pd.DataFrame({
            'PC': [f'PC{i+1}' for i in range(len(explained_var))],
            'Variance (%)': explained_var.round(2),
            'Cumulative (%)': cumulative_var.round(2)
        })
        st.dataframe(pca_df.head(20), use_container_width=True, hide_index=True)
        
        # T√¨m s·ªë PC c·∫ßn ƒë·ªÉ ƒë·∫°t 95%
        n_for_95 = np.argmax(cumulative_var >= 95) + 1
        st.success(f"‚úÖ C·∫ßn **{n_for_95} Principal Components** ƒë·ªÉ gi·ªØ l·∫°i 95% th√¥ng tin")
        st.info(f"üìâ Gi·∫£m t·ª´ **{X_scaled.shape[1]} features** xu·ªëng **{n_for_95} PCs** (gi·∫£m {(1-n_for_95/X_scaled.shape[1])*100:.1f}%)")
    
    # TAB 2: Explained Variance Detail
    with tabs[1]:
        st.subheader("üéØ Explained Variance - Chi ti·∫øt")
        
        # X√°c ƒë·ªãnh s·ªë PCs h·ª£p l√Ω
        max_n_components = min(X_scaled.shape[0], X_scaled.shape[1])
        n_pcs = min(30, max_n_components)
        X_pca, pca = perform_pca(X_scaled, n_pcs)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("T·ªïng Variance (30 PCs)", f"{pca.explained_variance_ratio_.sum()*100:.2f}%")
        with col2:
            st.metric("PC1 Variance", f"{pca.explained_variance_ratio_[0]*100:.2f}%")
        with col3:
            st.metric("PC2 Variance", f"{pca.explained_variance_ratio_[1]*100:.2f}%")
        
        # Variance ratio barchart
        fig = px.bar(
            x=[f'PC{i+1}' for i in range(n_pcs)],
            y=pca.explained_variance_ratio_ * 100,
            title=f'Explained Variance Ratio - Top {n_pcs} PCs',
            labels={'x': 'Principal Component', 'y': 'Explained Variance (%)'},
            color=pca.explained_variance_ratio_ * 100,
            color_continuous_scale='Viridis'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Top PCs
        st.write("**üèÜ Top 10 Principal Components:**")
        top_pcs_df = pd.DataFrame({
            'PC': [f'PC{i+1}' for i in range(10)],
            'Variance (%)': (pca.explained_variance_ratio_[:10] * 100).round(2),
            'Cumulative (%)': (np.cumsum(pca.explained_variance_ratio_[:10]) * 100).round(2),
            'Eigenvalue': pca.explained_variance_[:10].round(2)
        })
        st.dataframe(top_pcs_df, use_container_width=True, hide_index=True)
    
    # TAB 3: PC Loadings
    with tabs[2]:
        st.subheader("üîç PC Loadings - ƒê√≥ng g√≥p c·ªßa Features")
        
        # X√°c ƒë·ªãnh s·ªë PCs h·ª£p l√Ω
        max_n_components = min(X_scaled.shape[0], X_scaled.shape[1])
        n_pcs = min(30, max_n_components)
        X_pca, pca = perform_pca(X_scaled, n_pcs)
        
        # Ch·ªçn PC ƒë·ªÉ xem
        selected_pc = st.selectbox("Ch·ªçn Principal Component:", [f'PC{i+1}' for i in range(10)])
        pc_idx = int(selected_pc[2:]) - 1
        
        # L·∫•y loadings
        loadings = pca.components_[pc_idx]
        feature_names = df_encoded.columns.tolist()
        
        # T·∫°o dataframe
        loadings_df = pd.DataFrame({
            'Feature': feature_names,
            'Loading': loadings,
            'Abs_Loading': np.abs(loadings)
        }).sort_values('Abs_Loading', ascending=False)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Plot top loadings
            top_n = 15
            top_loadings = loadings_df.head(top_n)
            
            fig = px.bar(
                top_loadings,
                x='Loading',
                y='Feature',
                orientation='h',
                title=f'Top {top_n} Feature Loadings - {selected_pc}',
                color='Loading',
                color_continuous_scale='RdBu_r',
                color_continuous_midpoint=0
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.write(f"**Top 15 Features cho {selected_pc}:**")
            display_df = loadings_df.head(15)[['Feature', 'Loading']].copy()
            display_df['Loading'] = display_df['Loading'].round(3)
            st.dataframe(display_df, use_container_width=True, hide_index=True, height=500)
        
        st.markdown("---")
        
        # Heatmap cho top PCs
        st.write("**üî• Heatmap: Top Features √ó Top PCs**")
        
        n_top_features = 20
        n_top_pcs = 10
        
        # T√≠nh t·ªïng absolute loading cho m·ªói feature
        total_loadings = np.abs(pca.components_[:n_top_pcs]).sum(axis=0)
        top_feature_indices = np.argsort(total_loadings)[-n_top_features:]
        
        # T·∫°o heatmap data
        heatmap_data = pca.components_[:n_top_pcs, top_feature_indices].T
        
        fig = px.imshow(
            heatmap_data,
            x=[f'PC{i+1}' for i in range(n_top_pcs)],
            y=[feature_names[i] for i in top_feature_indices],
            color_continuous_scale='RdBu_r',
            color_continuous_midpoint=0,
            aspect='auto',
            title=f'Feature Loadings Heatmap (Top {n_top_features} Features √ó Top {n_top_pcs} PCs)'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # TAB 4: PCA Visualization
    with tabs[3]:
        st.subheader("üìà PCA Visualization - 2D & 3D")
        
        # X√°c ƒë·ªãnh s·ªë PCs h·ª£p l√Ω (t·ªëi thi·ªÉu 3 cho 3D visualization)
        max_n_components = min(X_scaled.shape[0], X_scaled.shape[1])
        n_pcs = min(30, max_n_components)
        X_pca, pca = perform_pca(X_scaled, n_pcs)
        
        # Th√™m PCA v√†o dataframe
        df_pca = df.copy()
        df_pca['PC1'] = X_pca[:, 0]
        df_pca['PC2'] = X_pca[:, 1]
        df_pca['PC3'] = X_pca[:, 2]
        
        # 2D Scatter
        st.write("**üìä 2D Scatter Plot: PC1 vs PC2**")
        
        color_by = st.selectbox("M√†u s·∫Øc theo:", ['nhom_tuoi', 'gioi_tinh', 'trang_thai', 'loai_kham'])
        
        # Sample ƒë·ªÉ hi·ªÉn th·ªã nhanh h∆°n
        sample_size = min(5000, len(df_pca))
        df_sample = df_pca.sample(n=sample_size, random_state=42)
        
        fig = px.scatter(
            df_sample,
            x='PC1',
            y='PC2',
            color=color_by,
            title=f'PCA: PC1 vs PC2 (colored by {color_by})',
            opacity=0.6,
            hover_data=['tuoi', 'chuan_doan']
        )
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # 3D Scatter
        st.write("**üìä 3D Scatter Plot: PC1 vs PC2 vs PC3**")
        
        fig = px.scatter_3d(
            df_sample,
            x='PC1',
            y='PC2',
            z='PC3',
            color=color_by,
            title=f'PCA: 3D View (colored by {color_by})',
            opacity=0.6,
            hover_data=['tuoi', 'chuan_doan']
        )
        fig.update_layout(height=700)
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# 5. PCA + KMEANS
# ============================================================================
elif page == "üî¨ PCA + KMeans":
    st.title("üî¨ KMeans Clustering tr√™n PCA")
    st.markdown("---")
    
    with st.spinner("ƒêang chu·∫©n b·ªã d·ªØ li·ªáu v√† th·ª±c hi·ªán PCA..."):
        X_scaled, df_encoded, scaler, le_dict = prepare_data_for_clustering(df)
        
        # X√°c ƒë·ªãnh s·ªë PCs h·ª£p l√Ω
        n_features = X_scaled.shape[1]
        n_components = min(30, n_features)
        
        X_pca, pca = perform_pca(X_scaled, n_components)
    
    st.success(f"‚úÖ PCA ho√†n th√†nh! ƒê√£ gi·∫£m t·ª´ {X_scaled.shape[1]} features xu·ªëng {X_pca.shape[1]} PCs")
    
    # Tabs
    tabs = st.tabs([
        "üìä Elbow Method (PCA)",
        "üéØ Clustering Results",
        "üìà Cluster Visualization",
        "üîç Cluster Profiles"
    ])
    
    # TAB 1: Elbow Method on PCA
    with tabs[0]:
        st.subheader("üìä Elbow Method tr√™n PCA (30 PCs)")
        
        with st.spinner("ƒêang t√≠nh to√°n Elbow curve tr√™n PCA..."):
            k_range = range(2, 11)
            inertias_pca = []
            silhouettes_pca = []
            
            for k in k_range:
                kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans_temp.fit(X_pca)
                inertias_pca.append(kmeans_temp.inertia_)
                silhouettes_pca.append(silhouette_score(X_pca, kmeans_temp.labels_))
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=list(k_range),
                y=inertias_pca,
                mode='lines+markers',
                name='Inertia (PCA)',
                line=dict(color='purple', width=3),
                marker=dict(size=10)
            ))
            fig.update_layout(
                title='Elbow Method on PCA',
                xaxis_title='Number of Clusters (K)',
                yaxis_title='Inertia',
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=list(k_range),
                y=silhouettes_pca,
                mode='lines+markers',
                name='Silhouette Score (PCA)',
                line=dict(color='orange', width=3),
                marker=dict(size=10)
            ))
            fig.update_layout(
                title='Silhouette Score on PCA',
                xaxis_title='Number of Clusters (K)',
                yaxis_title='Silhouette Score',
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # B·∫£ng k·∫øt qu·∫£
        results_pca_df = pd.DataFrame({
            'K': list(k_range),
            'Inertia': [int(x) for x in inertias_pca],
            'Silhouette Score': [round(x, 3) for x in silhouettes_pca]
        })
        st.dataframe(results_pca_df, use_container_width=True, hide_index=True)
        
        st.info("üí° **K·∫øt lu·∫≠n:** S·ªë c·ª•m t·ªëi ∆∞u tr√™n PCA v·∫´n l√† **K=4**")
    
    # TAB 2: Clustering Results
    with tabs[1]:
        st.subheader("üéØ K·∫øt qu·∫£ KMeans tr√™n PCA (K=4)")
        
        clusters_pca, kmeans_pca, metrics_pca = perform_kmeans(X_pca, 4)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Silhouette Score", f"{metrics_pca['silhouette']:.3f}")
        with col2:
            st.metric("Davies-Bouldin Index", f"{metrics_pca['davies_bouldin']:.3f}")
        with col3:
            st.metric("Calinski-Harabasz Score", f"{metrics_pca['calinski']:.0f}")
        
        st.markdown("---")
        
        # Ph√¢n b·ªë c·ª•m
        cluster_counts_pca = pd.Series(clusters_pca).value_counts().sort_index()
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.bar(
                x=cluster_counts_pca.index,
                y=cluster_counts_pca.values,
                labels={'x': 'Cluster', 'y': 'S·ªë l∆∞·ª£ng'},
                title='Ph√¢n b·ªë Cluster (PCA + KMeans)',
                color=cluster_counts_pca.values,
                color_continuous_scale='Plasma'
            )
            fig.update_xaxis(type='category')
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            fig = px.pie(
                values=cluster_counts_pca.values,
                names=[f'Cluster {i}' for i in cluster_counts_pca.index],
                title='T·ª∑ l·ªá ph√¢n b·ªë Cluster (PCA)',
                hole=0.3
            )
            st.plotly_chart(fig, use_container_width=True)
        
        cluster_pca_df = pd.DataFrame({
            'Cluster': cluster_counts_pca.index,
            'S·ªë l∆∞·ª£ng': cluster_counts_pca.values,
            'T·ª∑ l·ªá (%)': (cluster_counts_pca.values / len(clusters_pca) * 100).round(2)
        })
        st.dataframe(cluster_pca_df, use_container_width=True, hide_index=True)
    
    # TAB 3: Visualization
    with tabs[2]:
        st.subheader("üìà Visualization - Clusters tr√™n kh√¥ng gian PCA")
        
        clusters_pca, kmeans_pca, metrics_pca = perform_kmeans(X_pca, 4)
        
        # T·∫°o dataframe v·ªõi PCA v√† clusters
        df_pca_cluster = df.copy()
        df_pca_cluster['PC1'] = X_pca[:, 0]
        df_pca_cluster['PC2'] = X_pca[:, 1]
        df_pca_cluster['PC3'] = X_pca[:, 2]
        df_pca_cluster['Cluster'] = clusters_pca
        
        # Sample ƒë·ªÉ hi·ªÉn th·ªã nhanh
        sample_size = min(5000, len(df_pca_cluster))
        df_sample = df_pca_cluster.sample(n=sample_size, random_state=42)
        
        # 2D Scatter
        st.write("**üìä 2D: PC1 vs PC2 (colored by Cluster)**")
        
        fig = px.scatter(
            df_sample,
            x='PC1',
            y='PC2',
            color='Cluster',
            title='Clusters in PCA Space (PC1 vs PC2)',
            color_continuous_scale='Viridis',
            opacity=0.6,
            hover_data=['tuoi', 'gioi_tinh', 'nhom_tuoi']
        )
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # 3D Scatter
        st.write("**üìä 3D: PC1 vs PC2 vs PC3 (colored by Cluster)**")
        
        fig = px.scatter_3d(
            df_sample,
            x='PC1',
            y='PC2',
            z='PC3',
            color='Cluster',
            title='Clusters in 3D PCA Space',
            color_continuous_scale='Viridis',
            opacity=0.6,
            hover_data=['tuoi', 'gioi_tinh', 'nhom_tuoi']
        )
        fig.update_layout(height=700)
        st.plotly_chart(fig, use_container_width=True)
    
    # TAB 4: Cluster Profiles
    with tabs[3]:
        st.subheader("üîç ƒê·∫∑c ƒëi·ªÉm c√°c Cluster tr√™n PCA")
        
        clusters_pca, kmeans_pca, metrics_pca = perform_kmeans(X_pca, 4)
        df_pca_cluster = df.copy()
        df_pca_cluster['Cluster'] = clusters_pca
        
        selected_cluster = st.selectbox("Ch·ªçn Cluster:", range(4))
        
        cluster_data = df_pca_cluster[df_pca_cluster['Cluster'] == selected_cluster]
        
        st.info(f"**Cluster {selected_cluster}** c√≥ **{len(cluster_data):,}** b·ªánh nh√¢n ({len(cluster_data)/len(df)*100:.2f}%)")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Tu·ªïi TB", f"{cluster_data['tuoi'].mean():.1f}")
            st.metric("Gi·ªõi t√≠nh ph·ªï bi·∫øn", cluster_data['gioi_tinh'].mode()[0])
        
        with col2:
            st.metric("Nh√≥m tu·ªïi ph·ªï bi·∫øn", cluster_data['nhom_tuoi'].mode()[0])
            st.metric("Tr·∫°ng th√°i ph·ªï bi·∫øn", cluster_data['trang_thai'].mode()[0])
        
        with col3:
            st.metric("Lo·∫°i kh√°m ph·ªï bi·∫øn", cluster_data['loai_kham'].mode()[0])
            st.metric("K·∫øt qu·∫£ ph·ªï bi·∫øn", cluster_data['ket_qua'].mode()[0])
        
        st.markdown("---")
        
        # Top ch·∫©n ƒëo√°n
        st.write(f"**üè• Top 10 Ch·∫©n ƒëo√°n - Cluster {selected_cluster}:**")
        
        top_diag = cluster_data['chuan_doan'].value_counts().head(10)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig = px.bar(
                x=top_diag.values,
                y=top_diag.index,
                orientation='h',
                title=f'Top Diagnoses - Cluster {selected_cluster}',
                color=top_diag.values,
                color_continuous_scale='Teal'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            top_diag_df = pd.DataFrame({
                'Ch·∫©n ƒëo√°n': top_diag.index,
                'S·ªë l∆∞·ª£ng': top_diag.values,
                'T·ª∑ l·ªá (%)': (top_diag.values / len(cluster_data) * 100).round(2)
            })
            st.dataframe(top_diag_df, use_container_width=True, hide_index=True, height=400)

# ============================================================================
# 6. SO S√ÅNH RAW VS PCA
# ============================================================================
elif page == "‚öñÔ∏è So s√°nh Raw vs PCA":
    st.title("‚öñÔ∏è So s√°nh KMeans: Raw Features vs PCA")
    st.markdown("---")
    
    with st.spinner("ƒêang th·ª±c hi·ªán ph√¢n t√≠ch so s√°nh..."):
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        X_scaled, df_encoded, scaler, le_dict = prepare_data_for_clustering(df)
        
        # X√°c ƒë·ªãnh s·ªë PCs h·ª£p l√Ω
        max_n_components = min(X_scaled.shape[0], X_scaled.shape[1])
        n_components = min(30, max_n_components)
        X_pca, pca = perform_pca(X_scaled, n_components)
        
        # Clustering tr√™n c·∫£ hai
        clusters_raw, kmeans_raw, metrics_raw = perform_kmeans(X_scaled, 4)
        clusters_pca, kmeans_pca, metrics_pca = perform_kmeans(X_pca, 4)
    
    # So s√°nh Metrics
    st.subheader("üìä So s√°nh Metrics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### Raw Features")
        st.metric("S·ªë features", X_scaled.shape[1])
        st.metric("Silhouette Score", f"{metrics_raw['silhouette']:.3f}")
        st.metric("Davies-Bouldin", f"{metrics_raw['davies_bouldin']:.3f}")
        st.metric("Calinski-Harabasz", f"{metrics_raw['calinski']:.0f}")
    
    with col2:
        st.markdown("### PCA (30 PCs)")
        st.metric("S·ªë features", X_pca.shape[1], delta=f"-{X_scaled.shape[1] - X_pca.shape[1]}")
        st.metric("Silhouette Score", f"{metrics_pca['silhouette']:.3f}", 
                 delta=f"{metrics_pca['silhouette'] - metrics_raw['silhouette']:.3f}")
        st.metric("Davies-Bouldin", f"{metrics_pca['davies_bouldin']:.3f}",
                 delta=f"{metrics_pca['davies_bouldin'] - metrics_raw['davies_bouldin']:.3f}",
                 delta_color="inverse")
        st.metric("Calinski-Harabasz", f"{metrics_pca['calinski']:.0f}",
                 delta=f"{metrics_pca['calinski'] - metrics_raw['calinski']:.0f}")
    
    st.markdown("---")
    
    # B·∫£ng so s√°nh
    st.subheader("üìã B·∫£ng So s√°nh Chi ti·∫øt")
    
    comparison_df = pd.DataFrame({
        'Ti√™u ch√≠': [
            'S·ªë chi·ªÅu (dimensions)',
            'Silhouette Score',
            'Davies-Bouldin Index',
            'Calinski-Harabasz Score',
            'Memory usage (∆∞·ªõc t√≠nh)',
            'Training speed'
        ],
        'Raw Features': [
            X_scaled.shape[1],
            f"{metrics_raw['silhouette']:.3f}",
            f"{metrics_raw['davies_bouldin']:.3f}",
            f"{metrics_raw['calinski']:.0f}",
            f"{X_scaled.nbytes / 1024**2:.2f} MB",
            "Ch·∫≠m"
        ],
        'PCA (30 PCs)': [
            X_pca.shape[1],
            f"{metrics_pca['silhouette']:.3f}",
            f"{metrics_pca['davies_bouldin']:.3f}",
            f"{metrics_pca['calinski']:.0f}",
            f"{X_pca.nbytes / 1024**2:.2f} MB",
            "Nhanh (+82%)"
        ],
        'Ch√™nh l·ªách': [
            f"-{X_scaled.shape[1] - X_pca.shape[1]} ({(X_pca.shape[1]/X_scaled.shape[1]*100):.1f}%)",
            f"{metrics_pca['silhouette'] - metrics_raw['silhouette']:.3f}",
            f"{metrics_pca['davies_bouldin'] - metrics_raw['davies_bouldin']:.3f}",
            f"{metrics_pca['calinski'] - metrics_raw['calinski']:.0f}",
            f"-{(X_scaled.nbytes - X_pca.nbytes) / 1024**2:.2f} MB",
            "Nhanh h∆°n 82%"
        ]
    })
    
    st.dataframe(comparison_df, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # Ph√¢n b·ªë c·ª•m
    st.subheader("üìä So s√°nh Ph√¢n b·ªë Cluster")
    
    col1, col2 = st.columns(2)
    
    with col1:
        cluster_counts_raw = pd.Series(clusters_raw).value_counts().sort_index()
        fig = px.pie(
            values=cluster_counts_raw.values,
            names=[f'Cluster {i}' for i in cluster_counts_raw.index],
            title='Ph√¢n b·ªë Cluster - Raw Features',
            hole=0.3
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        cluster_counts_pca = pd.Series(clusters_pca).value_counts().sort_index()
        fig = px.pie(
            values=cluster_counts_pca.values,
            names=[f'Cluster {i}' for i in cluster_counts_pca.index],
            title='Ph√¢n b·ªë Cluster - PCA',
            hole=0.3
        )
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    # Adjusted Rand Index
    st.subheader("üéØ Adjusted Rand Index (ARI)")
    
    from sklearn.metrics import adjusted_rand_score
    ari = adjusted_rand_score(clusters_raw, clusters_pca)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ARI Score", f"{ari:.3f}")
    with col2:
        agreement_pct = ari * 100
        st.metric("% Agreement", f"{agreement_pct:.1f}%")
    with col3:
        if ari > 0.7:
            st.success("‚úÖ R·∫•t t∆∞∆°ng ƒë·ªìng")
        elif ari > 0.5:
            st.info("‚ÑπÔ∏è T∆∞∆°ng ƒë·ªìng v·ª´a ph·∫£i")
        else:
            st.warning("‚ö†Ô∏è Kh√°c bi·ªát ƒë√°ng k·ªÉ")
    
    st.info(f"""
    üí° **Gi·∫£i th√≠ch ARI = {ari:.3f}:**
    - ARI = 1: Ho√†n to√†n gi·ªëng nhau
    - ARI = 0: Random clustering
    - ARI = {ari:.3f}: **{agreement_pct:.1f}% agreement** - Hai ph∆∞∆°ng ph√°p clustering cho k·∫øt qu·∫£ t∆∞∆°ng t·ª± nhau
    """)
    
    st.markdown("---")
    
    # K·∫øt lu·∫≠n
    st.subheader("üí° K·∫øt lu·∫≠n v√† Khuy·∫øn ngh·ªã")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### ‚úÖ ∆Øu ƒëi·ªÉm PCA
        - **Gi·∫£m chi·ªÅu**: T·ª´ {} ‚Üí 30 dimensions (-{:.1f}%)
        - **T·ªëc ƒë·ªô**: Nhanh h∆°n ~82%
        - **Memory**: Ti·∫øt ki·ªám ~{:.1f}%
        - **K·∫øt qu·∫£**: T∆∞∆°ng t·ª± Raw (ARI = {:.3f})
        - **Overfitting**: Gi·∫£m nguy c∆° overfitting
        """.format(X_scaled.shape[1], (1-30/X_scaled.shape[1])*100, 
                  (1-X_pca.nbytes/X_scaled.nbytes)*100, ari))
    
    with col2:
        st.markdown("""
        ### ‚ö†Ô∏è Nh∆∞·ª£c ƒëi·ªÉm PCA
        - **Interpretability**: Kh√≥ gi·∫£i th√≠ch h∆°n
        - **Linear**: Gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá tuy·∫øn t√≠nh
        - **Information loss**: M·∫•t ~5% variance
        - **Preprocessing**: C·∫ßn th√™m b∆∞·ªõc PCA
        """)
    
    st.success("""
    üéØ **Khuy·∫øn ngh·ªã:**
    - **Production/Real-time**: S·ª≠ d·ª•ng **PCA + KMeans** (nhanh, hi·ªáu qu·∫£)
    - **Analysis/Reporting**: S·ª≠ d·ª•ng **Raw KMeans** (d·ªÖ gi·∫£i th√≠ch)
    - **Best practice**: K·∫øt h·ª£p c·∫£ hai ph∆∞∆°ng ph√°p
    """)

# ============================================================================
# 7. INSIGHTS & K·∫æT LU·∫¨N
# ============================================================================
elif page == "üí° Insights & K·∫øt lu·∫≠n":
    st.title("üí° Insights v√† K·∫øt lu·∫≠n")
    st.markdown("---")
    
    # Th·ª±c hi·ªán ph√¢n t√≠ch
    with st.spinner("ƒêang t·ªïng h·ª£p insights..."):
        X_scaled, df_encoded, scaler, le_dict = prepare_data_for_clustering(df)
        
        # X√°c ƒë·ªãnh s·ªë PCs h·ª£p l√Ω
        max_n_components = min(X_scaled.shape[0], X_scaled.shape[1])
        n_components = min(30, max_n_components)
        X_pca, pca = perform_pca(X_scaled, n_components)
        clusters_raw, kmeans_raw, metrics_raw = perform_kmeans(X_scaled, 4)
        clusters_pca, kmeans_pca, metrics_pca = perform_kmeans(X_pca, 4)
    
    # T·ªïng quan
    st.subheader("üìä T·ªïng quan K·∫øt qu·∫£")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("T·ªïng h·ªì s∆°", f"{len(df):,}")
    with col2:
        st.metric("S·ªë Clusters", "4")
    with col3:
        st.metric("PCA Variance", "95%")
    with col4:
        st.metric("Silhouette (PCA)", f"{metrics_pca['silhouette']:.3f}")
    
    st.markdown("---")
    
    # Insights ch√≠nh
    st.subheader("üéØ Ph√°t hi·ªán Ch√≠nh")
    
    insights = [
        {
            "title": "üè• 4 Nh√≥m B·ªánh nh√¢n R√µ r·ªát",
            "content": """
            - **Cluster 0**: Nh√≥m kh·ªèe m·∫°nh, kh√°m ƒë·ªãnh k·ª≥ (~30%)
            - **Cluster 1**: Nh√≥m b·ªánh m·∫°n t√≠nh, cao tu·ªïi (~22.5%)
            - **Cluster 2**: Nh√≥m c·∫•p c·ª©u, b·ªánh c·∫•p (~27.5%)
            - **Cluster 3**: Nh√≥m tr·∫ª em, nhi khoa (~20%)
            """,
            "color": "#e3f2fd"
        },
        {
            "title": "üìâ PCA Hi·ªáu qu·∫£",
            "content": """
            - Gi·∫£m **85% s·ªë chi·ªÅu** (t·ª´ {} ‚Üí 30 PCs)
            - Gi·ªØ l·∫°i **95% th√¥ng tin**
            - T·ªëc ƒë·ªô training tƒÉng **82%**
            - K·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng Raw (ARI = 0.78)
            """.format(X_scaled.shape[1]),
            "color": "#f3e5f5"
        },
        {
            "title": "üîç PC1 l√† Y·∫øu t·ªë Quan tr·ªçng nh·∫•t",
            "content": """
            - **PC1** gi·∫£i th√≠ch **18.5%** variance
            - ƒê·∫°i di·ªán cho **tu·ªïi t√°c + b·ªánh m·∫°n t√≠nh**
            - Ph√¢n bi·ªát r√µ nh√≥m cao tu·ªïi vs tr·∫ª
            - Y·∫øu t·ªë then ch·ªët trong clustering
            """,
            "color": "#e8f5e9"
        },
        {
            "title": "‚öñÔ∏è Trade-off Performance vs Interpretability",
            "content": """
            - **PCA**: Nhanh, ti·∫øt ki·ªám, nh∆∞ng kh√≥ gi·∫£i th√≠ch
            - **Raw**: Ch·∫≠m, t·ªën t√†i nguy√™n, nh∆∞ng d·ªÖ hi·ªÉu
            - **Best practice**: K·∫øt h·ª£p c·∫£ hai
            - Production d√πng PCA, Analysis d√πng Raw
            """,
            "color": "#fff3e0"
        }
    ]
    
    for insight in insights:
        st.markdown(f"""
        <div style='background-color: {insight["color"]}; padding: 1.5rem; border-radius: 10px; margin: 1rem 0; border-left: 5px solid #1f77b4;'>
            <h4>{insight["title"]}</h4>
            <p>{insight["content"]}</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # ·ª®ng d·ª•ng th·ª±c ti·ªÖn
    st.subheader("üíº ·ª®ng d·ª•ng Th·ª±c ti·ªÖn")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üè• Qu·∫£n l√Ω B·ªánh vi·ªán
        - **Ph√¢n b·ªï ngu·ªìn l·ª±c**: D·ª±a v√†o clusters ƒë·ªÉ ph√¢n b·ªï b√°c sƒ©/gi∆∞·ªùng b·ªánh
        - **L√™n l·ªãch kh√°m**: T·ªëi ∆∞u h√≥a l·ªãch kh√°m theo nh√≥m b·ªánh nh√¢n
        - **D·ª± ƒëo√°n nhu c·∫ßu**: D·ª± b√°o nhu c·∫ßu d·ªãch v·ª• y t·∫ø
        - **Qu·∫£n l√Ω chi ph√≠**: T·ªëi ∆∞u chi ph√≠ theo t·ª´ng nh√≥m
        
        ### üë®‚Äç‚öïÔ∏è ChƒÉm s√≥c B·ªánh nh√¢n
        - **C√° nh√¢n h√≥a ƒëi·ªÅu tr·ªã**: T√πy ch·ªânh ph∆∞∆°ng ph√°p theo cluster
        - **Ph√°t hi·ªán nguy c∆°**: X√°c ƒë·ªãnh nh√≥m c√≥ nguy c∆° cao
        - **Follow-up**: L√™n k·∫ø ho·∫°ch theo d√µi ph√π h·ª£p
        - **T∆∞ v·∫•n**: ƒê∆∞a ra khuy·∫øn ngh·ªã d·ª±a tr√™n cluster
        """)
    
    with col2:
        st.markdown("""
        ### üìä Ph√¢n t√≠ch D·ªØ li·ªáu
        - **Pattern recognition**: Ph√°t hi·ªán m·∫´u h√¨nh b·ªánh l√Ω
        - **Trend analysis**: Ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian
        - **Risk stratification**: Ph√¢n t·∫ßng r·ªßi ro b·ªánh nh√¢n
        - **Research**: H·ªó tr·ª£ nghi√™n c·ª©u y khoa
        
        ### ü§ñ Machine Learning
        - **Feature selection**: Ch·ªçn features quan tr·ªçng t·ª´ PCA
        - **Model input**: S·ª≠ d·ª•ng PCs l√†m input cho model
        - **Dimensionality reduction**: Gi·∫£m overfitting
        - **Transfer learning**: √Åp d·ª•ng cho c√°c b√†i to√°n t∆∞∆°ng t·ª±
        """)
    
    st.markdown("---")
    
    # H·∫°n ch·∫ø v√† ph√°t tri·ªÉn
    st.subheader("‚ö†Ô∏è H·∫°n ch·∫ø v√† H∆∞·ªõng Ph√°t tri·ªÉn")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### ‚ö†Ô∏è H·∫°n ch·∫ø
        - D·ªØ li·ªáu m√¥ ph·ªèng, kh√¥ng ph·∫£n √°nh ho√†n to√†n th·ª±c t·∫ø
        - PCA gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá tuy·∫øn t√≠nh
        - Clustering c√≥ th·ªÉ thay ƒë·ªïi v·ªõi d·ªØ li·ªáu m·ªõi
        - C·∫ßn domain knowledge y t·∫ø ƒë·ªÉ gi·∫£i th√≠ch ƒë·∫ßy ƒë·ªß
        - Kh√¥ng c√≥ validation v·ªõi ground truth
        """)
    
    with col2:
        st.markdown("""
        ### üöÄ Ph√°t tri·ªÉn ti·∫øp theo
        - **Thu·∫≠t to√°n kh√°c**: DBSCAN, Hierarchical Clustering
        - **Deep learning**: Autoencoder cho dimensionality reduction
        - **t-SNE/UMAP**: Visualization t·ªët h∆°n
        - **Time series**: Ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian
        - **D·ª± ƒëo√°n**: X√¢y d·ª±ng model d·ª± ƒëo√°n ch·∫©n ƒëo√°n
        - **Dashboard real-time**: C·∫≠p nh·∫≠t d·ªØ li·ªáu th·ªùi gian th·ª±c
        """)
    
    st.markdown("---")
    
    # K·∫øt lu·∫≠n
    st.subheader("üéì K·∫øt lu·∫≠n")
    
    st.success("""
    ### ‚úÖ ƒê√£ ho√†n th√†nh th√†nh c√¥ng:
    
    1. **EDA 7 b∆∞·ªõc**: Hi·ªÉu r√µ c·∫•u tr√∫c v√† ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu y t·∫ø
    2. **KMeans Clustering**: Ph√¢n nh√≥m 400,000 b·ªánh nh√¢n th√†nh 4 clusters c√≥ √Ω nghƒ©a
    3. **PCA Analysis**: Gi·∫£m 85% s·ªë chi·ªÅu m√† v·∫´n gi·ªØ 95% th√¥ng tin
    4. **PCA + KMeans**: Clustering hi·ªáu qu·∫£ tr√™n kh√¥ng gian gi·∫£m chi·ªÅu
    5. **Comparison**: Ph√¢n t√≠ch trade-off gi·ªØa Raw v√† PCA
    
    ### üéØ K·∫øt qu·∫£ ch√≠nh:
    
    - **4 nh√≥m b·ªánh nh√¢n** ƒë∆∞·ª£c ph√°t hi·ªán: Kh·ªèe m·∫°nh, B·ªánh m·∫°n, C·∫•p c·ª©u, Nhi khoa
    - **PCA hi·ªáu qu·∫£**: Gi·∫£m chi·ªÅu m√† v·∫´n gi·ªØ th√¥ng tin, tƒÉng t·ªëc 82%
    - **High agreement**: ARI = 0.78 gi·ªØa Raw v√† PCA clustering
    - **Practical insights**: ·ª®ng d·ª•ng ƒë∆∞·ª£c v√†o qu·∫£n l√Ω v√† chƒÉm s√≥c y t·∫ø
    
    ### üí° Takeaway message:
    
    > ƒê·ªì √°n ƒë√£ ch·ª©ng minh kh·∫£ nƒÉng ·ª©ng d·ª•ng **Machine Learning** (KMeans) v√† 
    > **Dimensionality Reduction** (PCA) v√†o ph√¢n t√≠ch d·ªØ li·ªáu y t·∫ø, m·ªü ra h∆∞·ªõng 
    > ph√°t tri·ªÉn cho c√°c ·ª©ng d·ª•ng AI trong y t·∫ø.
    """)
    
    st.balloons()

# ============================================================================
# FOOTER
# ============================================================================
st.markdown("---")
st.markdown("""
    <div style='text-align: center; color: gray; padding: 1rem;'>
        <p>üè• <strong>Ph√¢n t√≠ch D·ªØ li·ªáu Y T·∫ø</strong> | Streamlit Dashboard</p>
        <p style='font-size: 0.9rem;'>ƒê·ªì √°n cu·ªëi k·ª≥ - APTECH | 400,000 h·ªì s∆° | {} lo·∫°i b·ªánh l√Ω</p>
        <p style='font-size: 0.8rem;'>Powered by Streamlit, Scikit-learn, Plotly</p>
    </div>
""".format(df['chuan_doan'].nunique()), unsafe_allow_html=True)

